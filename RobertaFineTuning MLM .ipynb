{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2221bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd20a0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install pandas\n",
    "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4c1ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,json\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "from transformers import RobertaTokenizer, RobertaForMaskedLM\n",
    "from transformers import LineByLineTextDataset, DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import urlextract\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20aec51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor = urlextract.URLExtract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0065f146",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_diff_lang(string):\n",
    "    return any(ord(char) > 127 for char in string)\n",
    "\n",
    "def check_usable(data):\n",
    "    temp=data.split(\"\\n\")\n",
    "    length = len(data.split(\" \"))\n",
    "    urls = extractor.find_urls(data)\n",
    "\n",
    "    # to remove url\n",
    "    for url in urls:\n",
    "        data = re.sub(re.escape(url), '', data)\n",
    "        \n",
    "    avoid_sentence=False\n",
    "    if length <= 2:\n",
    "        for word in data.split(\" \"):\n",
    "            if any(char.isdigit() for char in word):\n",
    "                print(word,\" here \",data)\n",
    "                avoid_sentence = True\n",
    "                break\n",
    "    if check_diff_lang(data):\n",
    "        avoid_sentence = True\n",
    "        \n",
    "    if temp == None or temp==\"\":\n",
    "        avoid_sentence = True\n",
    "    if avoid_sentence:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# Function to read the datas from the given folder and make it as a dataset \n",
    "def read_files(path,file):\n",
    "    count = 0\n",
    "    for filename in sorted(os.listdir(path)):\n",
    "        if filename.endswith('.jsonl'):\n",
    "            file_path = os.path.join(path, filename)\n",
    "            with open(file_path, 'r') as f:\n",
    "                for line in f:\n",
    "                    data = json.loads(line)\n",
    "                    data= data[\"docstring\"].split(\"\\n\")[0]\n",
    "                    if check_usable(data):\n",
    "                        continue\n",
    "                    data = data.encode('unicode_escape').decode()\n",
    "                    file.write(fr\"{data}\"+\"\\n\")\n",
    "                    count+=1\n",
    "            print(file_path,\" \",count)\n",
    "    return count\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45e720d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prep_data(folder_paths,text_file):\n",
    "    tot_count=0\n",
    "    with open(text_file, 'w', encoding='utf-8') as f:\n",
    "        for path in folder_paths:\n",
    "            tot_count = tot_count + read_files(path,f)\n",
    "    return tot_count\n",
    "            \n",
    "#prep_data([r'./java/java/final/jsonl/train',r'./python/python/final/jsonl/train'],'train_data.txt')\n",
    "#prep_data([r'./java/java/final/jsonl/test',r'./python/python/final/jsonl/test'],'test_data.txt')\n",
    "#prep_data([r'./java/java/final/jsonl/valid',r'./python/python/final/jsonl/valid'],'valid_data.txt')\n",
    "#paths to load the dataset\n",
    "print(\"Number of training dataset sentences = \",prep_data([r'./python/python/final/jsonl/train'],'train_data.txt'))\n",
    "print(\"Number of test dataset sentences = \",prep_data([r'./python/python/final/jsonl/test'],'test_data.txt'))\n",
    "print(\"Number of validation dataset sentences = \",prep_data([r'./python/python/final/jsonl/valid'],'valid_data.txt'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570fd40e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# put 'sentence-transformers/stsb-roberta-base' in place of roberta-base to change the model\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "model = RobertaForMaskedLM.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8970529a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1d70dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the datasets and apply the tokenizer of the model\n",
    "train_dataset = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"train_data.txt\",\n",
    "    block_size=128,\n",
    ")\n",
    "valid_dataset = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"valid_data.txt\",\n",
    "    block_size=128,\n",
    ")\n",
    "test_dataset = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"test_data.txt\",\n",
    "    block_size=56,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b642df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "#to reduce the dataset for fine tuning, currently considering 15% of dataset\n",
    "train_dataset=train_dataset[:int(math.floor(len(train_dataset)*0.15))]\n",
    "valid_dataset=valid_dataset[:int(math.floor(len(valid_dataset)*0.15))]\n",
    "test_dataset=test_dataset[:int(math.floor(len(test_dataset)*0.15))]\n",
    "print(len(train_dataset), len(valid_dataset), len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0648b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the data collator to perform MLM task\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")\n",
    "\n",
    "# Setting up the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    eval_steps=10000,\n",
    "    save_steps=50000,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=500,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_accumulation_steps=3,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "torch.cuda.memory_summary(device=None, abbreviated=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4804bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65213661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainning the model\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    data_collator=data_collator,\n",
    "    eval_dataset=valid_dataset,\n",
    "    compute_metrics=lambda p: {\"accuracy\": (p.predictions.argmax(axis=-1) == p.label_ids).mean().item()},\n",
    ")\n",
    "\n",
    "trainer.model.to(device)\n",
    "trainer.train()\n",
    "\n",
    "model.save_pretrained('./fin_modelmlm')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad762348",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Caution to test the dataset, the system will try to assign big memory, if not able to allocate, it will through an error.\n",
    "#Even if this evaluation fails, thats fine, the idea is to train it for a few shots and save the model and test it in our main \n",
    "#evaluation which is in Evaluation script.\n",
    "del(train_dataset)\n",
    "del(valid_dataset)\n",
    "torch.cuda.empty_cache()\n",
    "eval_result = trainer.evaluate(test_dataset)\n",
    "\n",
    "print(eval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3eda295",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
